{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "### Statistical Learning\n",
    "\n",
    "Author: Anas Barakat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last cell contains the code to obtain the best score. \n",
    "Cells before show all the methods tested on the data before finding the optimal score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Applications/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#for neural network models \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from time import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performance criterion\n",
    "def compute_pred_score(y_true, y_pred):\n",
    "    y_pred_unq =  np.unique(y_pred)\n",
    "    for i in y_pred_unq:\n",
    "        if((i != -1) & (i!= 1) & (i!= 0) ):\n",
    "            raise ValueError('The predictions can contain only -1, 1, or 0!')\n",
    "    y_comp = y_true * y_pred\n",
    "    score = float(10*np.sum(y_comp == -1) + np.sum(y_comp == 0))\n",
    "    score /= y_comp.shape[0]\n",
    "    return score\n",
    "\n",
    "X_train_fname = 'training_templates.csv'\n",
    "y_train_fname = 'training_labels.txt'\n",
    "X_test_fname  = 'testing_templates.csv'\n",
    "X_train = pd.read_csv(X_train_fname, sep=',', header=None).values\n",
    "X_test  = pd.read_csv(X_test_fname,  sep=',', header=None).values\n",
    "y_train = np.loadtxt(y_train_fname, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Attempt \n",
    "#3 #Improve the predictive power of the model by tuning the parameters/ increasing number of estimators \n",
    "#time / quality tradeoff, the bigger the better \n",
    "n_estimators = 500\n",
    "\n",
    "# Feature selection after using 'random forest's 'feature_importance'\n",
    
    "#X_train = X_train[:, [19, 34, 44, 57, 69, 74, 77, 79, 86, 87, 102, 116]]\n",
    "#X_test  = X_test[:, [19, 34, 44, 57, 69, 74, 77, 79, 86, 87, 102, 116]]\n",
    "\n",
    "#X_train = X_train[:,[1, 6, 13, 14, 19, 25, 32, 34, 40, 44, 49, 52, 57, 65, 67, 69, 74, 76, 77, 79, 86, 87, 102, 105, 106, 108, 116, 121, 122, 126]\n",
    "#]\n",
    "#X_test = X_test[:,[1, 6, 13, 14, 19, 25, 32, 34, 40, 44, 49, 52, 57, 65, 67, 69, 74, 76, 77, 79, 86, 87, 102, 105, 106, 108, 116, 121, 122, 126]\n",
    "#]\n",
    "\n",
    "# RF fitting\n",
    "model = RandomForestClassifier(n_estimators=n_estimators, max_features = 'log2',max_depth = None,n_jobs = -1, min_samples_leaf = 1)\n",
    "clfRF = model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_train =  clfRF.predict(X_train)\n",
    "#print(clfRF.feature_importances_)\n",
    "#plt.plot(clfRF.feature_importances_)\n",
    "\n",
    "# feature selection \n",
    "#importantFeatures = []\n",
    "#for k in range(128):\n",
    "#    if clfRF.feature_importances_[k]>0.010:\n",
    "#        importantFeatures += [k]\n",
    "#print(importantFeatures)\n",
    "\n",
    "# Compute the score\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print('Score sur le train : %s' % score)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clfRF.predict(X_test)\n",
    "\n",
    "proba = clfRF.predict_proba(X_test)\n",
    "print(proba[0:10,])\n",
    "\n",
    "# predict 0 when the probabilities of predicting -1 and 1 are close \n",
    "for k in range(1,y_pred.shape[0]):\n",
    "    if 0.35<proba[k,1]<0.65:\n",
    "        y_pred[k] = 0\n",
    "\n",
    "np.savetxt('y_pred.txt', y_pred, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grid search to find optimal parameters \n",
    "\n",
    "#finding max_features optimal parameter by CV \n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "param_grid = { \n",
    "    #'n_estimators': [200, 700],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "    'min_samples_leaf': [1,10,20,50,100]\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gradient boosting classifier - as seen in the lecture \n",
    "n_estimators = 200\n",
    "\n",
    "\n",
    "# RF fitting\n",
    "model = GradientBoostingClassifier(n_estimators=n_estimators, max_features = \"log2\", min_samples_leaf=1)\n",
    "clfGB = model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_train =  clfGB.predict(X_train)\n",
    "\n",
    "# Compute the score\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print('Score sur le train : %s' % score)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clfGB.predict(X_test)\n",
    "\n",
    "proba = clfGB.predict_proba(X_test)\n",
    "print(proba[0:10,])\n",
    "\n",
    "# predict 0 when the probabilities of predicting -1 and 1 are close \n",
    "#\n",
    "for k in range(1,y_pred.shape[0]):\n",
    "    if 0.4<proba[k,1]<0.6:\n",
    "        y_pred[k] = 0\n",
    "\n",
    "np.savetxt('y_pred.txt', y_pred, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#score 0.498352165725\n",
    "# SVC fitting\n",
    "model = LinearSVC(C=25)\n",
    "# puis SVC avec noyau polynomial et rbf \n",
    "\n",
    "#X_train = X_train[:,[1, 6, 13, 14, 19, 25, 32, 34, 40, 44, 49, 52, 57, 65, 67, 69, 74, 76, 77, 79, 86, 87, 102, 105, 106, 108, 116, 121, 122, 126]\n",
    "#]\n",
    "#X_test = X_test[:,[1, 6, 13, 14, 19, 25, 32, 34, 40, 44, 49, 52, 57, 65, 67, 69, 74, 76, 77, 79, 86, 87, 102, 105, 106, 108, 116, 121, 122, 126]\n",
    "#]\n",
    "\n",
    "clfSVM = model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_train =  clfSVM.predict(X_train)\n",
    "\n",
    "# Compute the score\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print('Score sur le train : %s' % score)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clfSVM.predict(X_test)\n",
    "\n",
    "dist = clfSVM.decision_function(X_test)\n",
    "print(dist[0:10,])\n",
    "\n",
    "# predict 0 when the probabilities of predicting -1 and 1 are close/ thresholding \n",
    "#\n",
    "for k in range(1,y_pred.shape[0]):\n",
    "    if dist[k]<=0.25:\n",
    "        y_pred[k] = 0\n",
    "\n",
    "np.savetxt('y_pred.txt', y_pred, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78bc4d945dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m model = Sequential([\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden1_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "#neural networks with keras \n",
    "from keras import optimizers\n",
    "# define vars\n",
    "input_num_units = 128\n",
    "hidden1_num_units = 40\n",
    "hidden2_num_units = 40\n",
    "hidden3_num_units = 40\n",
    "hidden4_num_units = 40\n",
    "hidden5_num_units = 40\n",
    "hidden6_num_units = 40\n",
    "output_num_units = 1\n",
    "\n",
    "#epochs = 5 #score 0.265301318267\n",
    "#epochs = 10 #score  0.243644067797\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "weight_constraint = 3\n",
    "\n",
    "# create model\n",
    "model = Sequential([\n",
    "    Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu', kernel_constraint=maxnorm(weight_constraint)),\n",
    "        Dropout(dropout_ratio),\n",
    "    Dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='relu', kernel_constraint=maxnorm(weight_constraint)),\n",
    "        Dropout(dropout_ratio),\n",
    "        \n",
    "    Dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='relu', kernel_constraint=maxnorm(weight_constraint)),\n",
    "        Dropout(dropout_ratio),\n",
    "    Dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='relu', kernel_constraint=maxnorm(weight_constraint)),\n",
    "        Dropout(dropout_ratio),\n",
    "    Dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='relu', kernel_constraint=maxnorm(weight_constraint)),\n",
    "        Dropout(dropout_ratio),\n",
    "        \n",
    "    Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# compile the model with necessary attributes\n",
    "#sgd = SGD(lr=0.001, momentum=0.8, decay=0.0, nesterov=False)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "\n",
    "for k in range(1,y_train.shape[0]):\n",
    "    if y_train[k]==-1:\n",
    "        y_train[k] = 0\n",
    "\n",
    "\n",
    "print(\"starting fitting\")\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, nb_epoch= epochs, batch_size= batch_size,  verbose=1)\n",
    "print(\"fitting finished\")\n",
    "\n",
    "# calculate predictions\n",
    "print(\"calculate predictions xtest\")\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "for k in range(1,y_pred.shape[0]):\n",
    "    if y_pred[k]==0:\n",
    "        y_pred[k] = -1\n",
    "\n",
    "print(\"calculate predictions probas\")\n",
    "proba = model.predict_proba(X_test)\n",
    "\n",
    "# predict 0 when the probabilities of predicting -1 and 1 are close \n",
    "#\n",
    "for k in range(1,y_pred.shape[0]):\n",
    "    if 0.4<proba[k]<0.6:\n",
    "        y_pred[k] = 0\n",
    "\n",
    "np.savetxt('y_pred2.txt', y_pred, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score sur le train : 0.0013257575757575758\n",
      "[[  2.55640227e-02   9.74435977e-01]\n",
      " [  9.99999353e-01   6.47292669e-07]\n",
      " [  2.88919041e-06   9.99997111e-01]\n",
      " [  1.42858500e-07   9.99999857e-01]\n",
      " [  9.99994155e-01   5.84519747e-06]\n",
      " [  5.19577260e-05   9.99948042e-01]\n",
      " [  9.67327701e-01   3.26722991e-02]\n",
      " [  9.99999828e-01   1.72037433e-07]\n",
      " [  1.66336829e-04   9.99833663e-01]\n",
      " [  5.19927051e-07   9.99999480e-01]]\n",
      "execution time: 3499.8385169506073\n"
     ]
    }
   ],
   "source": [
    "# Trying bagging \n",
    "#Multi Layer Perceptron \n",
    "#0.175376647834\n",
    "ls = 200\n",
    "#ls2 = 150 #100\n",
    "#ls3 = 75 #50\n",
    "#ls1(250) ls2(100) ls3 (50)\n",
    "\n",
    "time1 = time()\n",
    "clfMLP = MLPClassifier(solver='adam',alpha=7e-8, hidden_layer_sizes=(ls), random_state=1, max_iter=400)\n",
    "#learning_rate_init = 1e-6\n",
    "#warm_start =True, learning_rate ='adaptive'\n",
    "\n",
    "clfMLP = BaggingClassifier(clfMLP, n_estimators= 50, max_features=0.8, n_jobs=-1)\n",
    "\n",
    "clfMLP.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clfMLP.predict(X_test)\n",
    "\n",
    "# Prediction\n",
    "y_pred_train =  clfMLP.predict(X_train)\n",
    "\n",
    "# Compute the score\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print('Score sur le train : %s' % score)\n",
    "\n",
    "proba = clfMLP.predict_proba(X_test)\n",
    "print(proba[0:10,])\n",
    "\n",
    "for k in range(1,y_pred.shape[0]):\n",
    "    if 0.001<proba[k,1]<0.999:\n",
    "        y_pred[k] = 0\n",
    "        \n",
    "time2 = time()\n",
    "\n",
    "print(\"execution time:\", time2 - time1)\n",
    "np.savetxt('y_pred.txt', y_pred, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls = 200\n",
    "#ls2 = 150 #100\n",
    "#ls3 = 75 #50\n",
    "#ls1(250) ls2(100) ls3 (50)\n",
    "\n",
    "time1 = time()\n",
    "clfMLP = MLPClassifier(solver='adam',alpha=7e-8, hidden_layer_sizes=(ls), random_state=1, max_iter=400)\n",
    "#learning_rate_init = 1e-6\n",
    "#warm_start =True, learning_rate ='adaptive'\n",
    "\n",
    "clfMLP.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clfMLP.predict(X_test)\n",
    "\n",
    "# Prediction\n",
    "y_pred_train =  clfMLP.predict(X_train)\n",
    "\n",
    "# Compute the score\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print('Score sur le train : %s' % score)\n",
    "\n",
    "proba = clfMLP.predict_proba(X_test)\n",
    "print(proba[0:10,])\n",
    "\n",
    "for k in range(1,y_pred.shape[0]):\n",
    "    if 0.001<proba[k,1]<0.999:\n",
    "        y_pred[k] = 0\n",
    "        \n",
    "time2 = time()\n",
    "\n",
    "print(\"execution time:\", time2 - time1)\n",
    "np.savetxt('y_pred.txt', y_pred, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
